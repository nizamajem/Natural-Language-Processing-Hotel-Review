# -*- coding: utf-8 -*-
"""Muhamad Nizam Azmi _ medium ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bbhdi5IWjklsrsL-kT0AbKi3bt45CT-O
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, Callback
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as plt_image
import math
from google.colab import files, drive
# %matplotlib inline

import string, re, nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from google.colab import drive

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

df = pd.read_csv('tripadvisor_hotel_reviews.csv')
df

list_rating_1 = df[df.Rating == 1]
list_rating_2 = df[df.Rating == 2]
list_rating_3 = df[df.Rating == 3]
list_rating_4 = df[df.Rating == 4]
list_rating_5 = df[df.Rating == 5]

list_rating_1 = list_rating_1[0:1000]
list_rating_2 = list_rating_2[0:1000]
list_rating_3 = list_rating_3[0:1000]
list_rating_4 = list_rating_4[0:1000]
list_rating_5 = list_rating_5[0:1000]

new_df = pd.concat([list_rating_1, list_rating_2, list_rating_3, list_rating_4, list_rating_5], axis=0)

new_df = new_df.sample(frac = 1)

new_df.head(10)

sns.countplot(x = 'Rating', data = new_df)

case folding
stopword
stemming
tokenizing


# Remove numbers
def remove_numbers(data):
    return re.sub('[0-9]+','',data)
new_df['Review']=df['Review'].apply(lambda x: remove_numbers(x))

# Remove Punctuation
def remove_punctuation(text):
    no_punct=[words for words in text if words not in string.punctuation]
    words_wo_punct=''.join(no_punct)
    return words_wo_punct
new_df['Review']=df['Review'].apply(lambda x: remove_punctuation(x))

# Remove Stop Words
stopword = stopwords.words('english')
def remove_stopwords(text):
    return(' '.join([w for w in text.split() if w not in stopword ]))
new_df.Review = df.Review.apply(lambda x: remove_stopwords(x))

# Word Lemmatizer
lemmatizer = WordNetLemmatizer()
def lem(data):
    pos_dict = {'N': wn.NOUN, 'V': wn.VERB, 'J': wn.ADJ, 'R': wn.ADV}
    return(' '.join([lemmatizer.lemmatize(w,pos_dict.get(t, wn.NOUN)) for w,t in nltk.pos_tag(data.split())]))
new_df.Review = df.Review.apply(lambda x: lem(x))

new_df.head()

Rating = pd.get_dummies(new_df.Rating)
new_df = pd.concat([new_df, Rating], axis=1)
new_df = new_df.drop(columns='Rating')
# new_df = new_df[1500:4500]
new_df

review = new_df['Review'].values
label = new_df[[1, 2, 3, 4, 5]].values

from sklearn.model_selection import train_test_split

review_train, review_test, label_train, label_test = train_test_split(review, label, test_size=0.2)

import tensorflow as tf

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.75 and logs.get('accuracy')>0.75):
      print("\nAkurasi telah mencapai >90%! dan Akurasi validation telah mencapai >75%")
      self.model.stop_training = True
callbacks = myCallback()

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000, oov_token='-')
tokenizer.fit_on_texts(review_train)
tokenizer.fit_on_texts(review_test)

sequens_train = tokenizer.texts_to_sequences(review_train)
sequens_test = tokenizer.texts_to_sequences(review_test)

from tensorflow.keras.preprocessing.sequence import pad_sequences

pad_train = pad_sequences(sequens_train)
pad_test = pad_sequences(sequens_test)

# membuat arsitektur 

import tensorflow as tf

from tensorflow.keras import Sequential

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM),
    tf.keras.layers.SpatialDropout1D(0.2),
    tf.keras.layers.LSTM(100, activation = 'softmax'),
    tf.keras.layers.Dense(5, activation='softmax')
])


model.summary()

from tensorflow import keras

from tensorflow.keras import layers

opt = keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

# snippet of using the LearningRateScheduler callback
from keras.callbacks import LearningRateScheduler

 
def my_learning_rate(epoch, lrate):
	return lrate
 
lrs = LearningRateScheduler(my_learning_rate)
num_epochs = 10
history = model.fit(pad_train, label_train, epochs=num_epochs, 
                    validation_data=(pad_test, label_test), callbacks=[lrs])

# Plot akurasi model
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

# Plot loss model
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

